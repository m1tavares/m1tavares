{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GroupProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m1tavares/m1tavares/blob/master/GroupProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMxaS6RXatYK"
      },
      "source": [
        "  #importing packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        " \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib\n",
        "plt.style.use('ggplot')\n",
        "from matplotlib.pyplot import figure\n",
        " \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        " \n",
        "#import data\n",
        "news = pd.read_csv('/content/online_news_popularity.csv')\n",
        "shop = pd.read_csv('/content/online_shoppers_intention.csv')\n",
        " \n",
        " \n",
        "#print coloumn headers and the first 5 rows to see data frame.\n",
        "print(news.columns.values)\n",
        "print(shop.columns.values)\n",
        "shop.head()\n",
        " \n",
        "#creating bins for each share ranges\n",
        "binx = [0,500,1000,1500,2000,2500,3000, 5000,10000,15000,20000, 500000]\n",
        "#converting \"shares\" from continious to categorical\n",
        "pd.cut(news[' shares'], bins=binx).value_counts(sort=False)\n",
        "news['shares_group'] = pd.cut(news[' shares'], bins=binx)\n",
        "#converting categorical back into int to be used in our learning methods\n",
        "news['shares_group'] = pd.factorize(news['shares_group'])[0] + 1\n",
        " \n",
        "news.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaOqWKqRvkDi"
      },
      "source": [
        "#checking for missing data points. None were found... data exploration\n",
        "for col in news.columns:\n",
        "  pct_missing = np.mean(news[col].isnull())\n",
        "  print('{} - {}%'.format(col, round(pct_missing*100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YegR2kYjvkjz"
      },
      "source": [
        "#test/train\n",
        "\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import cm\n",
        "#all features\n",
        "feature_names= [' n_tokens_title', ' n_tokens_content',\n",
        " ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
        " ' num_hrefs',' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
        " ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
        " ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
        " ' data_channel_is_socmed', ' data_channel_is_tech',\n",
        " ' data_channel_is_world' ,' kw_min_min' ,' kw_max_min', ' kw_avg_min',\n",
        " ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg', ' kw_max_avg',\n",
        " ' kw_avg_avg', ' self_reference_min_shares', ' self_reference_max_shares',\n",
        " ' self_reference_avg_sharess', ' weekday_is_monday', ' weekday_is_tuesday',\n",
        " ' weekday_is_wednesday', ' weekday_is_thursday', ' weekday_is_friday',\n",
        " ' weekday_is_saturday' ,' weekday_is_sunday' ,' is_weekend' ,' LDA_00',\n",
        " ' LDA_01', ' LDA_02', ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
        " ' global_sentiment_polarity', ' global_rate_positive_words',\n",
        " ' global_rate_negative_words', ' rate_positive_words',\n",
        " ' rate_negative_words', ' avg_positive_polarity', ' min_positive_polarity',\n",
        " ' max_positive_polarity', ' avg_negative_polarity',\n",
        " ' min_negative_polarity', ' max_negative_polarity' ,' title_subjectivity',\n",
        " ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
        " ' abs_title_sentiment_polarity']\n",
        "x = news[feature_names]\n",
        "y = news['shares_group']\n",
        "\n",
        "\n",
        "\n",
        "#selected few features.\n",
        "shop_feature_names=['BounceRates', 'ExitRates', 'PageValues']\n",
        "x1 = shop[shop_feature_names]\n",
        "y1 = shop['Revenue']\n",
        "\n",
        "newsx_train, newsx_test, newsy_train, newsy_test = train_test_split(x,y, test_size=0.3, random_state=0)\n",
        "shopx_train, shopx_test, shopy_train, shopy_test = train_test_split(x1,y1, test_size=0.3, random_state=0)\n",
        "\n",
        "#scaling data set\n",
        "scaler = MinMaxScaler(feature_range=(-1,1)).fit(newsx_train)\n",
        "newsx_train = scaler.fit_transform(newsx_train)\n",
        "newsx_test = scaler.transform(newsx_test)\n",
        "\n",
        "shopx_train = scaler.fit_transform(shopx_train)\n",
        "shopx_test = scaler.transform(shopx_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3IW7dckxGq3"
      },
      "source": [
        "#SVM\n",
        "svmnews=SVC()\n",
        "svmshop=SVC()\n",
        "\n",
        "svmshop.fit(shopx_train, shopy_train)\n",
        "print('Accuracy of SVM for shop train data {:.2f}'.format(svmshop.score(shopx_train, shopy_train)))\n",
        "print('Accuracy of SVM for shop test data {:.2f}'.format(svmshop.score(shopx_test, shopy_test)))\n",
        "\n",
        "svmnews.fit(newsx_train, newsy_train)\n",
        "\n",
        "#long run time, low accuracy. \n",
        "print('Accuracy of SVM for news train data {:.2f}'.format(svmnews.score(newsx_train, newsy_train)))\n",
        "print('Accuracy of SVM for news test data {:.2f}'.format(svmnews.score(newsx_test, newsy_test)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTM0G6QcxPJG"
      },
      "source": [
        "#knn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(newsx_train, newsy_train)\n",
        "print('Accuracy of K-NN for news train data: {:.2f} '.format(knn.score(newsx_train, newsy_train)))\n",
        "print('Accuracy of K-NN for news test data: {:.2f} '.format(knn.score(newsx_test, newsy_test)))\n",
        "\n",
        "knn.fit(shopx_train, shopy_train)\n",
        "print('Accuracy of K-NN for shop train data: {:.2f} '.format(knn.score(shopx_train, shopy_train)))\n",
        "print('Accuracy of K-NN for shop test data: {:.2f} '.format(knn.score(shopx_test, shopy_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoaa_CQZxQkR"
      },
      "source": [
        "#decision\n",
        "clfnews = DecisionTreeClassifier().fit(newsx_train, newsy_train)\n",
        "clfshop = DecisionTreeClassifier().fit(shopx_train, shopy_train)\n",
        "\n",
        "\n",
        "#news data\n",
        "print('Accuracy of Decision Tree classifier on news training set: {:.2f}'\n",
        "     .format(clfnews.score(newsx_train, newsy_train)))\n",
        "print('Accuracy of Decision Tree classifier on news test set: {:.2f}'\n",
        "     .format(clfnews.score(newsx_test, newsy_test)))\n",
        "\n",
        "#shop data\n",
        "print('Accuracy of Decision Tree classifier on shop training set: {:.2f}'\n",
        "     .format(clfshop.score(shopx_train, shopy_train)))\n",
        "print('Accuracy of Decision Tree classifier on shop test set: {:.2f}'\n",
        "     .format(clfshop.score(shopx_test, shopy_test)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfFS6dndlYSN"
      },
      "source": [
        "#confusion matrix K-NN for shop\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = knn.predict(shopx_test)\n",
        "print(confusion_matrix(shopy_test, pred))\n",
        "print(classification_report(shopy_test, pred))\n",
        "\n",
        "pred1 = knn.predict(shopx_train)\n",
        "print(confusion_matrix(shopy_train, pred1))\n",
        "print(classification_report(shopy_train, pred1))\n",
        "\n",
        "#confusion matrix K-NN for news\n",
        "#refitting model to news...\n",
        "knn.fit(newsx_train, newsy_train)\n",
        "\n",
        "predx = knn.predict(newsx_test)\n",
        "print(confusion_matrix(newsy_test, predx))\n",
        "print(classification_report(newsy_test, predx)) \n",
        "\n",
        "predy = knn.predict(newsx_train)\n",
        "print(confusion_matrix(newsy_train, predy))\n",
        "print(classification_report(newsy_train, predy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-qc5d0e5Fpc"
      },
      "source": [
        "pred2 = svmshop.predict(shopx_test)\n",
        "print(confusion_matrix(shopy_test, pred2))\n",
        "print(classification_report(shopy_test, pred2))\n",
        "\n",
        "pred3 = svmshop.predict(shopx_train)\n",
        "print(confusion_matrix(shopy_train, pred3))\n",
        "print(classification_report(shopy_train, pred3))\n",
        "\n",
        "\n",
        "pred4 = svmnews.predict(newsx_test)\n",
        "print(confusion_matrix(newsy_test, pred4))\n",
        "print(classification_report(newsy_test, pred4)) \n",
        "\n",
        "pred5 = svmnews.predict(newsx_train)\n",
        "print(confusion_matrix(newsy_train, pred5))\n",
        "print(classification_report(newsy_train, pred5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpuqfpYC8zn9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "pred6 = clfnews.predict(newsx_test)\n",
        "print(confusion_matrix(newsy_test, pred6))\n",
        "print(classification_report(newsy_test, pred6)) \n",
        "\n",
        "pred7 = clfnews.predict(newsx_train)\n",
        "print(confusion_matrix(newsy_train, pred7))\n",
        "print(classification_report(newsy_train, pred7))\n",
        "\n",
        "pred8 = clfshop.predict(shopx_test)\n",
        "print(confusion_matrix(shopy_test, pred8))\n",
        "print(classification_report(shopy_test, pred8))\n",
        "\n",
        "pred9 = clfshop.predict(shopx_train)\n",
        "print(confusion_matrix(shopy_train, pred9))\n",
        "print(classification_report(shopy_train, pred9))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}